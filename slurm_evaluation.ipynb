{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "from __future__ import annotations\n",
    "\n",
    "from itertools import product\n",
    "\n",
    "import os\n",
    "\n",
    "from eval_utils import Dataset, Results, arguments, do_evaluations, METHODS, METRICS, eval_method, set_seed\n",
    "\n",
    "import tabpfn.scripts.tabular_baselines as tb\n",
    "from tabpfn.scripts.tabular_metrics import (calculate_score, time_metric)\n",
    "\n",
    "from submitit.submitit import SlurmExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_evaluations(args: argparse.Namespace, datasets, slurm_executer: SlurmExecutor = None) -> Results:\n",
    "    results = {}\n",
    "    jobs = {}\n",
    "    for seed, method, metric, time, split in product(\n",
    "        args.seeds,\n",
    "        args.methods,\n",
    "        args.optimization_metrics,\n",
    "        args.times,\n",
    "        range(0, args.splits),\n",
    "    ):\n",
    "        set_seed(seed=seed)\n",
    "        metric_f = METRICS[metric]\n",
    "        metric_name = tb.get_scoring_string(metric_f, usage=\"\")\n",
    "        key = f\"{method}_time_{time}{metric_name}_split_{split}_seed_{seed}\"\n",
    "\n",
    "        if slurm_executer is None:\n",
    "            results[key] = eval_method(\n",
    "            datasets=datasets,\n",
    "            label=method,\n",
    "            result_path=args.result_path,\n",
    "            classifier_evaluator=METHODS[method],\n",
    "            eval_positions=args.eval_positions,  # It's a constant basically\n",
    "            fetch_only=args.fetch_only,\n",
    "            verbose=args.verbose,\n",
    "            max_time=time,\n",
    "            metric_used=metric_f,\n",
    "            split=split,\n",
    "            seed=seed,\n",
    "            overwrite=args.overwrite,\n",
    "        )\n",
    "        else:\n",
    "            jobs[key] = slurm_executer.submit(slurm_executer.submit(eval_method,\n",
    "            datasets=datasets,\n",
    "            label=method,\n",
    "            result_path=args.result_path,\n",
    "            classifier_evaluator=METHODS[method],\n",
    "            eval_positions=args.eval_positions,  # It's a constant basically\n",
    "            fetch_only=args.fetch_only,\n",
    "            verbose=args.verbose,\n",
    "            max_time=time,\n",
    "            metric_used=metric_f,\n",
    "            split=split,\n",
    "            seed=seed,\n",
    "            overwrite=args.overwrite))\n",
    "\n",
    "    return results, jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_options = {\n",
    "    'result_path': None,\n",
    "    'times': [30], #3600],\n",
    "    'seeds': [896], # , 125, 624, 438, 706],\n",
    "    'splits': 5,\n",
    "    'validation_datasets': [13], # None,\n",
    "    'test_datasets': [39], # None,\n",
    "    'optimization_metrics': ['roc'],\n",
    "    'recorded_metrics': [\"roc\", \"cross_entropy\", \"acc\", \"brier_score\", \"ece\"],\n",
    "    'methods': [\n",
    "        'svm_default'],\n",
    "    #     ,\n",
    "    #     'gradient_boosting',\n",
    "    #     'knn',\n",
    "    #     'gp',\n",
    "    #     'lightgbm',\n",
    "    #     'xgb',\n",
    "    #     'random_forest',\n",
    "    #     'logistic',\n",
    "    #     'svm_default',\n",
    "    #     'gradient_boosting_default',\n",
    "    #     'gp_default',\n",
    "    #     'lightgbm_default',\n",
    "    #     'xgb_default',\n",
    "    #     'rf_default',\n",
    "    #     'transformer_cpu_N_1',\n",
    "    #     'transformer_cpu_N_8',\n",
    "    #     'transformer_cpu_N_32'\n",
    "        \n",
    "    # ],\n",
    "    'bptt': 2000,\n",
    "    'overwrite': False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BoschSlurmExecutor(SlurmExecutor):\n",
    "    def _make_submission_command(self, submission_file_path):\n",
    "        return [\"sbatch\", str(submission_file_path), '--bosch']\n",
    "\n",
    "base_path = os.path.join('/work/dlclarge1/rkohli-results_tabpfn_180/results_1667931216')\n",
    "log_folder = os.path.join(base_path, \"log_test/%j\")\n",
    "\n",
    "slurm_executer = BoschSlurmExecutor(folder=log_folder)\n",
    "slurm_executer.update_parameters(time=30,\n",
    "                     partition=\"bosch_cpu-cascadelake\",\n",
    "                     mem_per_cpu=6000,\n",
    "                     nodes=1,\n",
    "                     cpus_per_task=1,\n",
    "                     ntasks_per_node=1,\n",
    "                    #  setup=['export MKL_THREADING_LAYER=GNU']\n",
    "                    ) #  mldlc_gpu-rtx2080\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = argparse.Namespace(**experiment_options)\n",
    "\n",
    "args.result_path = base_path\n",
    "\n",
    "if not args.validation_datasets:\n",
    "    args.validation_datasets = \"cc_valid\"\n",
    "\n",
    "if not args.test_datasets:\n",
    "    args.test_datasets = \"cc_test\"\n",
    "\n",
    "# We need to create some directories for this to work\n",
    "out_dir = os.path.join(args.result_path, \"results\", \"tabular\", \"multiclass\")\n",
    "os.mkdir(out_dir,\n",
    "    parents=True, exist_ok=True\n",
    ")\n",
    "\n",
    "# We ignore the flags datasets\n",
    "filter_f = lambda d: d.name != \"flags\"  # noqa: ignore\n",
    "\n",
    "valid_datasets = Dataset.fetch(args.validation_datasets, only=filter_f)\n",
    "test_datasets = Dataset.fetch(args.test_datasets, only=filter_f)\n",
    "\n",
    "all_datasets = valid_datasets + test_datasets\n",
    "all_datasets = all_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results, jobs = do_evaluations(args, all_datasets)\n",
    "# for "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_as_lists = [d.as_list() for d in all_datasets]\n",
    "\n",
    "# This will update the results in place\n",
    "for metric in args.recorded_metrics:\n",
    "    metric_f = METRICS[metric]\n",
    "    calculate_score(\n",
    "        metric=metric_f,\n",
    "        name=metric,\n",
    "        global_results=results,\n",
    "        ds=datasets_as_lists,\n",
    "        eval_positions=args.eval_positions,\n",
    "    )\n",
    "\n",
    "# We also get the times\n",
    "calculate_score(\n",
    "    metric=time_metric,\n",
    "    name=\"time\",\n",
    "    global_results=results,\n",
    "    ds=datasets_as_lists,\n",
    "    eval_positions=args.eval_positions,\n",
    ")\n",
    "final_results = Results.from_dict(\n",
    "        results,\n",
    "        datasets=all_datasets,\n",
    "        recorded_metrics=args.recorded_metrics + [\"time\"],\n",
    "    )\n",
    "final_results.df.to_csv(os.path.join(out_dir, \"results.csv\"), index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('tabpfn_x86')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6c23330ed2c2568eecc1af03316607ee9f6b9b4b9024c49236381988f63f008f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
